{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "66fda854-5fca-416e-acd9-3d5d3fa7332e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the extracted folder:\n",
      "BuzzFeed_fake_news_content.csv\n",
      "BuzzFeed_real_news_content.csv\n",
      "combined_news_dataset.csv\n",
      "PolitiFact_fake_news_content.csv\n",
      "PolitiFact_real_news_content.csv\n",
      "Loading dataset: BuzzFeed_fake_news_content.csv\n",
      "Columns in BuzzFeed_fake_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Loading dataset: PolitiFact_fake_news_content.csv\n",
      "Columns in PolitiFact_fake_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Loading dataset: BuzzFeed_real_news_content.csv\n",
      "Columns in BuzzFeed_real_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Loading dataset: PolitiFact_real_news_content.csv\n",
      "Columns in PolitiFact_real_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Sample rows from the combined dataset:\n",
      "                                                text  label\n",
      "0  I woke up this morning to find a variation of ...      1\n",
      "1  Former President Bill Clinton and his Clinton ...      1\n",
      "2  After collapsing just before trying to step in...      1\n",
      "3  Donald Trump is, well, deplorable. He’s sugges...      1\n",
      "4                    Website is Down For Maintenance      1\n",
      "\n",
      "Accuracy: 0.4015748031496063\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.41      0.39        58\n",
      "           1       0.44      0.39      0.42        69\n",
      "\n",
      "    accuracy                           0.40       127\n",
      "   macro avg       0.40      0.40      0.40       127\n",
      "weighted avg       0.41      0.40      0.40       127\n",
      "\n",
      "✅ Model and vectorizer saved successfully!\n",
      "✅ Combined dataset saved to: Combined_News_Datasets\\combined_news_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "                                                  ###THIS IS LOGICAL REGRESSION####\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pickle\n",
    "\n",
    "# Step 1: Define the folder path\n",
    "folder_name = \"Combined_News_Datasets\"  # Folder containing your datasets\n",
    "\n",
    "# Step 2: Verify the files in the folder\n",
    "extracted_files = os.listdir(folder_name)\n",
    "print(\"Files in the extracted folder:\")\n",
    "for file in extracted_files:\n",
    "    print(file)\n",
    "\n",
    "# Step 3: Load and label the datasets\n",
    "dataset_files = {\n",
    "    \"BuzzFeed_fake_news_content.csv\": 1,\n",
    "    \"PolitiFact_fake_news_content.csv\": 1,\n",
    "    \"BuzzFeed_real_news_content.csv\": 0,\n",
    "    \"PolitiFact_real_news_content.csv\": 0,\n",
    "}\n",
    "\n",
    "datasets = []\n",
    "for file, label in dataset_files.items():\n",
    "    file_path = os.path.join(folder_name, file)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading dataset: {file}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Columns in {file}: {df.columns.tolist()}\")\n",
    "        \n",
    "        if 'text' in df.columns:\n",
    "            df = df[['text']]\n",
    "            df['label'] = label\n",
    "            datasets.append(df)\n",
    "        else:\n",
    "            print(f\"⚠️ 'text' column not found in {file}. Skipping this file.\")\n",
    "    else:\n",
    "        print(f\"⚠️ Dataset not found: {file}\")\n",
    "\n",
    "# Step 4: Combine all datasets into one DataFrame\n",
    "df = pd.concat(datasets, ignore_index=True)\n",
    "\n",
    "# Step 5: Handle missing values\n",
    "df = df.dropna(subset=['text', 'label'])\n",
    "\n",
    "print(\"Sample rows from the combined dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 6: Prepare data for model training\n",
    "X = df['text'].astype(str)\n",
    "y = df['label']\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the model and vectorizer\n",
    "with open('fake_news_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "    pickle.dump(vectorizer, model_file)\n",
    "\n",
    "print(\"✅ Model and vectorizer saved successfully!\")\n",
    "\n",
    "# Step 7: Save the combined dataset to a CSV\n",
    "combined_csv_path = os.path.join(folder_name, \"combined_news_dataset.csv\")\n",
    "df.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "print(f\"✅ Combined dataset saved to: {combined_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "58ddc676-6339-4ee7-8c0e-f502240fee2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the extracted folder:\n",
      "BuzzFeed_fake_news_content.csv\n",
      "BuzzFeed_real_news_content.csv\n",
      "combined_news_dataset.csv\n",
      "PolitiFact_fake_news_content.csv\n",
      "PolitiFact_real_news_content.csv\n",
      "Loading dataset: BuzzFeed_fake_news_content.csv\n",
      "Columns in BuzzFeed_fake_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Loading dataset: PolitiFact_fake_news_content.csv\n",
      "Columns in PolitiFact_fake_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Loading dataset: BuzzFeed_real_news_content.csv\n",
      "Columns in BuzzFeed_real_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Loading dataset: PolitiFact_real_news_content.csv\n",
      "Columns in PolitiFact_real_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Sample rows from the combined dataset:\n",
      "                                                text  label\n",
      "0  I woke up this morning to find a variation of ...      1\n",
      "1  Former President Bill Clinton and his Clinton ...      1\n",
      "2  After collapsing just before trying to step in...      1\n",
      "3  Donald Trump is, well, deplorable. He’s sugges...      1\n",
      "4                    Website is Down For Maintenance      1\n",
      "\n",
      "Accuracy: 0.3937007874015748\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.43      0.39        58\n",
      "           1       0.43      0.36      0.39        69\n",
      "\n",
      "    accuracy                           0.39       127\n",
      "   macro avg       0.40      0.40      0.39       127\n",
      "weighted avg       0.40      0.39      0.39       127\n",
      "\n",
      "✅ SVM model and vectorizer saved successfully!\n",
      "✅ Combined dataset saved to: Combined_News_Datasets\\combined_news_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "                                                               ####SVM######\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pickle\n",
    "\n",
    "# Step 1: Define the folder path\n",
    "folder_name = \"Combined_News_Datasets\"  # Folder containing your datasets\n",
    "\n",
    "# Step 2: Verify the files in the folder\n",
    "extracted_files = os.listdir(folder_name)\n",
    "print(\"Files in the extracted folder:\")\n",
    "for file in extracted_files:\n",
    "    print(file)\n",
    "\n",
    "# Step 3: Load and label the datasets\n",
    "dataset_files = {\n",
    "    \"BuzzFeed_fake_news_content.csv\": 1,\n",
    "    \"PolitiFact_fake_news_content.csv\": 1,\n",
    "    \"BuzzFeed_real_news_content.csv\": 0,\n",
    "    \"PolitiFact_real_news_content.csv\": 0,\n",
    "}\n",
    "\n",
    "datasets = []\n",
    "for file, label in dataset_files.items():\n",
    "    file_path = os.path.join(folder_name, file)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading dataset: {file}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Columns in {file}: {df.columns.tolist()}\")\n",
    "        \n",
    "        if 'text' in df.columns:\n",
    "            df = df[['text']]\n",
    "            df['label'] = label\n",
    "            datasets.append(df)\n",
    "        else:\n",
    "            print(f\"⚠️ 'text' column not found in {file}. Skipping this file.\")\n",
    "    else:\n",
    "        print(f\"⚠️ Dataset not found: {file}\")\n",
    "\n",
    "# Step 4: Combine all datasets into one DataFrame\n",
    "df = pd.concat(datasets, ignore_index=True)\n",
    "\n",
    "# Step 5: Handle missing values\n",
    "df = df.dropna(subset=['text', 'label'])\n",
    "\n",
    "print(\"Sample rows from the combined dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 6: Prepare data for model training\n",
    "X = df['text'].astype(str)\n",
    "y = df['label']\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the SVM model\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the model and vectorizer\n",
    "with open('fake_news_model_svm.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "    pickle.dump(vectorizer, model_file)\n",
    "\n",
    "print(\"✅ SVM model and vectorizer saved successfully!\")\n",
    "\n",
    "# Step 7: Save the combined dataset to a CSV\n",
    "combined_csv_path = os.path.join(folder_name, \"combined_news_dataset.csv\")\n",
    "df.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "print(f\"✅ Combined dataset saved to: {combined_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e80a3da7-d46c-466e-94b9-3dd6b0ce8acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the extracted folder:\n",
      "BuzzFeed_fake_news_content.csv\n",
      "BuzzFeed_real_news_content.csv\n",
      "combined_news_dataset.csv\n",
      "PolitiFact_fake_news_content.csv\n",
      "PolitiFact_real_news_content.csv\n",
      "Loading dataset: BuzzFeed_fake_news_content.csv\n",
      "Columns in BuzzFeed_fake_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Loading dataset: PolitiFact_fake_news_content.csv\n",
      "Columns in PolitiFact_fake_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Loading dataset: BuzzFeed_real_news_content.csv\n",
      "Columns in BuzzFeed_real_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Loading dataset: PolitiFact_real_news_content.csv\n",
      "Columns in PolitiFact_real_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Sample rows from the combined dataset:\n",
      "                                                text  label\n",
      "0  I woke up this morning to find a variation of ...      1\n",
      "1  Former President Bill Clinton and his Clinton ...      1\n",
      "2  After collapsing just before trying to step in...      1\n",
      "3  Donald Trump is, well, deplorable. He’s sugges...      1\n",
      "4                    Website is Down For Maintenance      1\n",
      "\n",
      "Accuracy: 0.3779527559055118\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.34      0.34        58\n",
      "           1       0.42      0.41      0.41        69\n",
      "\n",
      "    accuracy                           0.38       127\n",
      "   macro avg       0.38      0.38      0.38       127\n",
      "weighted avg       0.38      0.38      0.38       127\n",
      "\n",
      "✅ Linear classifier model and vectorizer saved successfully!\n",
      "✅ Combined dataset saved to: Combined_News_Datasets\\combined_news_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "                                               ######LINEAR CLASSIFICATION######\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pickle\n",
    "\n",
    "# Step 1: Define the folder path\n",
    "folder_name = \"Combined_News_Datasets\"  # Folder containing your datasets\n",
    "\n",
    "# Step 2: Verify the files in the folder\n",
    "extracted_files = os.listdir(folder_name)\n",
    "print(\"Files in the extracted folder:\")\n",
    "for file in extracted_files:\n",
    "    print(file)\n",
    "\n",
    "# Step 3: Load and label the datasets\n",
    "dataset_files = {\n",
    "    \"BuzzFeed_fake_news_content.csv\": 1,\n",
    "    \"PolitiFact_fake_news_content.csv\": 1,\n",
    "    \"BuzzFeed_real_news_content.csv\": 0,\n",
    "    \"PolitiFact_real_news_content.csv\": 0,\n",
    "}\n",
    "\n",
    "datasets = []\n",
    "for file, label in dataset_files.items():\n",
    "    file_path = os.path.join(folder_name, file)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading dataset: {file}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Columns in {file}: {df.columns.tolist()}\")\n",
    "        \n",
    "        if 'text' in df.columns:\n",
    "            df = df[['text']]\n",
    "            df['label'] = label\n",
    "            datasets.append(df)\n",
    "        else:\n",
    "            print(f\"⚠️ 'text' column not found in {file}. Skipping this file.\")\n",
    "    else:\n",
    "        print(f\"⚠️ Dataset not found: {file}\")\n",
    "\n",
    "# Step 4: Combine all datasets into one DataFrame\n",
    "df = pd.concat(datasets, ignore_index=True)\n",
    "\n",
    "# Step 5: Handle missing values\n",
    "df = df.dropna(subset=['text', 'label'])\n",
    "\n",
    "print(\"Sample rows from the combined dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 6: Prepare data for model training\n",
    "X = df['text'].astype(str)\n",
    "y = df['label']\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the linear classifier (SGDClassifier)\n",
    "model = SGDClassifier(loss='log_loss', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the model and vectorizer\n",
    "with open('fake_news_model_linear.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "    pickle.dump(vectorizer, model_file)\n",
    "\n",
    "print(\"✅ Linear classifier model and vectorizer saved successfully!\")\n",
    "\n",
    "# Step 7: Save the combined dataset to a CSV\n",
    "combined_csv_path = os.path.join(folder_name, \"combined_news_dataset.csv\")\n",
    "df.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "print(f\"✅ Combined dataset saved to: {combined_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6fa6551d-fc6f-4ae6-a415-3ab2ea978c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the extracted folder:\n",
      "BuzzFeed_fake_news_content.csv\n",
      "BuzzFeed_real_news_content.csv\n",
      "combined_news_dataset.csv\n",
      "PolitiFact_fake_news_content.csv\n",
      "PolitiFact_real_news_content.csv\n",
      "Loading dataset: BuzzFeed_fake_news_content.csv\n",
      "Columns in BuzzFeed_fake_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Loading dataset: PolitiFact_fake_news_content.csv\n",
      "Columns in PolitiFact_fake_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Loading dataset: BuzzFeed_real_news_content.csv\n",
      "Columns in BuzzFeed_real_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Loading dataset: PolitiFact_real_news_content.csv\n",
      "Columns in PolitiFact_real_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Sample rows from the combined dataset:\n",
      "                                                text  label\n",
      "0  I woke up this morning to find a variation of ...      1\n",
      "1  Former President Bill Clinton and his Clinton ...      1\n",
      "2  After collapsing just before trying to step in...      1\n",
      "3  Donald Trump is, well, deplorable. He’s sugges...      1\n",
      "4                    Website is Down For Maintenance      1\n",
      "\n",
      "Accuracy: 0.4645669291338583\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.45      0.43        58\n",
      "           1       0.51      0.48      0.49        69\n",
      "\n",
      "    accuracy                           0.46       127\n",
      "   macro avg       0.46      0.46      0.46       127\n",
      "weighted avg       0.47      0.46      0.47       127\n",
      "\n",
      "✅ Random Forest classifier model and vectorizer saved successfully!\n",
      "✅ Combined dataset saved to: Combined_News_Datasets\\combined_news_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "                                                     ####FOREST CLASSIFICATION####\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pickle\n",
    "\n",
    "# Step 1: Define the folder path\n",
    "folder_name = \"Combined_News_Datasets\"  # Folder containing your datasets\n",
    "\n",
    "# Step 2: Verify the files in the folder\n",
    "extracted_files = os.listdir(folder_name)\n",
    "print(\"Files in the extracted folder:\")\n",
    "for file in extracted_files:\n",
    "    print(file)\n",
    "\n",
    "# Step 3: Load and label the datasets\n",
    "dataset_files = {\n",
    "    \"BuzzFeed_fake_news_content.csv\": 1,\n",
    "    \"PolitiFact_fake_news_content.csv\": 1,\n",
    "    \"BuzzFeed_real_news_content.csv\": 0,\n",
    "    \"PolitiFact_real_news_content.csv\": 0,\n",
    "}\n",
    "\n",
    "datasets = []\n",
    "for file, label in dataset_files.items():\n",
    "    file_path = os.path.join(folder_name, file)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading dataset: {file}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Columns in {file}: {df.columns.tolist()}\")\n",
    "        \n",
    "        if 'text' in df.columns:\n",
    "            df = df[['text']]\n",
    "            df['label'] = label\n",
    "            datasets.append(df)\n",
    "        else:\n",
    "            print(f\"⚠️ 'text' column not found in {file}. Skipping this file.\")\n",
    "    else:\n",
    "        print(f\"⚠️ Dataset not found: {file}\")\n",
    "\n",
    "# Step 4: Combine all datasets into one DataFrame\n",
    "df = pd.concat(datasets, ignore_index=True)\n",
    "\n",
    "# Step 5: Handle missing values\n",
    "df = df.dropna(subset=['text', 'label'])\n",
    "\n",
    "print(\"Sample rows from the combined dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 6: Prepare data for model training\n",
    "X = df['text'].astype(str)\n",
    "y = df['label']\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the Random Forest classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the model and vectorizer\n",
    "with open('fake_news_model_forest.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "    pickle.dump(vectorizer, model_file)\n",
    "\n",
    "print(\"✅ Random Forest classifier model and vectorizer saved successfully!\")\n",
    "\n",
    "# Step 7: Save the combined dataset to a CSV\n",
    "combined_csv_path = os.path.join(folder_name, \"combined_news_dataset.csv\")\n",
    "df.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "print(f\"✅ Combined dataset saved to: {combined_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9f1ad99c-3f89-42ea-9cbf-ab0f82697259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\keert\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\keert\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\keert\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b7079fd5-e8b0-4639-a5a7-821f007fa476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the extracted folder:\n",
      "BuzzFeed_fake_news_content.csv\n",
      "BuzzFeed_real_news_content.csv\n",
      "combined_news_dataset.csv\n",
      "PolitiFact_fake_news_content.csv\n",
      "PolitiFact_real_news_content.csv\n",
      "Loading dataset: BuzzFeed_fake_news_content.csv\n",
      "Columns in BuzzFeed_fake_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Loading dataset: PolitiFact_fake_news_content.csv\n",
      "Columns in PolitiFact_fake_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Loading dataset: BuzzFeed_real_news_content.csv\n",
      "Columns in BuzzFeed_real_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Loading dataset: PolitiFact_real_news_content.csv\n",
      "Columns in PolitiFact_real_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Sample rows from the combined dataset:\n",
      "                                                text  label\n",
      "0  I woke up this morning to find a variation of ...      1\n",
      "1  Former President Bill Clinton and his Clinton ...      1\n",
      "2  After collapsing just before trying to step in...      1\n",
      "3  Donald Trump is, well, deplorable. He’s sugges...      1\n",
      "4                    Website is Down For Maintenance      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keert\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:48:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.4251968503937008\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.40      0.39        58\n",
      "           1       0.47      0.45      0.46        69\n",
      "\n",
      "    accuracy                           0.43       127\n",
      "   macro avg       0.42      0.42      0.42       127\n",
      "weighted avg       0.43      0.43      0.43       127\n",
      "\n",
      "✅ XGBoost classifier model and vectorizer saved successfully!\n",
      "✅ Combined dataset saved to: Combined_News_Datasets\\combined_news_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pickle\n",
    "\n",
    "# Step 1: Define the folder path\n",
    "folder_name = \"Combined_News_Datasets\"  # Folder containing your datasets\n",
    "\n",
    "# Step 2: Verify the files in the folder\n",
    "extracted_files = os.listdir(folder_name)\n",
    "print(\"Files in the extracted folder:\")\n",
    "for file in extracted_files:\n",
    "    print(file)\n",
    "\n",
    "# Step 3: Load and label the datasets\n",
    "dataset_files = {\n",
    "    \"BuzzFeed_fake_news_content.csv\": 1,\n",
    "    \"PolitiFact_fake_news_content.csv\": 1,\n",
    "    \"BuzzFeed_real_news_content.csv\": 0,\n",
    "    \"PolitiFact_real_news_content.csv\": 0,\n",
    "}\n",
    "\n",
    "datasets = []\n",
    "for file, label in dataset_files.items():\n",
    "    file_path = os.path.join(folder_name, file)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading dataset: {file}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Columns in {file}: {df.columns.tolist()}\")\n",
    "        \n",
    "        if 'text' in df.columns:\n",
    "            df = df[['text']]\n",
    "            df['label'] = label\n",
    "            datasets.append(df)\n",
    "        else:\n",
    "            print(f\"⚠️ 'text' column not found in {file}. Skipping this file.\")\n",
    "    else:\n",
    "        print(f\"⚠️ Dataset not found: {file}\")\n",
    "\n",
    "# Step 4: Combine all datasets into one DataFrame\n",
    "df = pd.concat(datasets, ignore_index=True)\n",
    "\n",
    "# Step 5: Handle missing values\n",
    "df = df.dropna(subset=['text', 'label'])\n",
    "\n",
    "print(\"Sample rows from the combined dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 6: Prepare data for model training\n",
    "X = df['text'].astype(str)\n",
    "y = df['label']\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the XGBoost classifier\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the model and vectorizer\n",
    "with open('fake_news_model_xgboost.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "    pickle.dump(vectorizer, model_file)\n",
    "\n",
    "print(\"✅ XGBoost classifier model and vectorizer saved successfully!\")\n",
    "\n",
    "# Step 7: Save the combined dataset to a CSV\n",
    "combined_csv_path = os.path.join(folder_name, \"combined_news_dataset.csv\")\n",
    "df.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "print(f\"✅ Combined dataset saved to: {combined_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5578f242-f5a2-4d16-9dc7-8dcdb7915e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the extracted folder:\n",
      "BuzzFeed_fake_news_content.csv\n",
      "BuzzFeed_real_news_content.csv\n",
      "combined_news_dataset.csv\n",
      "PolitiFact_fake_news_content.csv\n",
      "PolitiFact_real_news_content.csv\n",
      "Loading dataset: BuzzFeed_fake_news_content.csv\n",
      "Columns in BuzzFeed_fake_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Loading dataset: PolitiFact_fake_news_content.csv\n",
      "Columns in PolitiFact_fake_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Loading dataset: BuzzFeed_real_news_content.csv\n",
      "Columns in BuzzFeed_real_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Loading dataset: PolitiFact_real_news_content.csv\n",
      "Columns in PolitiFact_real_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Sample rows from the combined dataset:\n",
      "                                                text  label\n",
      "0  I woke up this morning to find a variation of ...      1\n",
      "1  Former President Bill Clinton and his Clinton ...      1\n",
      "2  After collapsing just before trying to step in...      1\n",
      "3  Donald Trump is, well, deplorable. He’s sugges...      1\n",
      "4                    Website is Down For Maintenance      1\n",
      "\n",
      "Accuracy: 0.36220472440944884\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.40      0.36        58\n",
      "           1       0.40      0.33      0.36        69\n",
      "\n",
      "    accuracy                           0.36       127\n",
      "   macro avg       0.36      0.36      0.36       127\n",
      "weighted avg       0.37      0.36      0.36       127\n",
      "\n",
      "✅ Decision Tree classifier model and vectorizer saved successfully!\n",
      "✅ Combined dataset saved to: Combined_News_Datasets\\combined_news_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pickle\n",
    "\n",
    "# Step 1: Define the folder path\n",
    "folder_name = \"Combined_News_Datasets\"  # Folder containing your datasets\n",
    "\n",
    "# Step 2: Verify the files in the folder\n",
    "extracted_files = os.listdir(folder_name)\n",
    "print(\"Files in the extracted folder:\")\n",
    "for file in extracted_files:\n",
    "    print(file)\n",
    "\n",
    "# Step 3: Load and label the datasets\n",
    "dataset_files = {\n",
    "    \"BuzzFeed_fake_news_content.csv\": 1,\n",
    "    \"PolitiFact_fake_news_content.csv\": 1,\n",
    "    \"BuzzFeed_real_news_content.csv\": 0,\n",
    "    \"PolitiFact_real_news_content.csv\": 0,\n",
    "}\n",
    "\n",
    "datasets = []\n",
    "for file, label in dataset_files.items():\n",
    "    file_path = os.path.join(folder_name, file)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading dataset: {file}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Columns in {file}: {df.columns.tolist()}\")\n",
    "        \n",
    "        if 'text' in df.columns:\n",
    "            df = df[['text']]\n",
    "            df['label'] = label\n",
    "            datasets.append(df)\n",
    "        else:\n",
    "            print(f\"⚠️ 'text' column not found in {file}. Skipping this file.\")\n",
    "    else:\n",
    "        print(f\"⚠️ Dataset not found: {file}\")\n",
    "\n",
    "# Step 4: Combine all datasets into one DataFrame\n",
    "df = pd.concat(datasets, ignore_index=True)\n",
    "\n",
    "# Step 5: Handle missing values\n",
    "df = df.dropna(subset=['text', 'label'])\n",
    "\n",
    "print(\"Sample rows from the combined dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 6: Prepare data for model training\n",
    "X = df['text'].astype(str)\n",
    "y = df['label']\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the Decision Tree classifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the model and vectorizer\n",
    "with open('fake_news_model_decision_tree.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "    pickle.dump(vectorizer, model_file)\n",
    "\n",
    "print(\"✅ Decision Tree classifier model and vectorizer saved successfully!\")\n",
    "\n",
    "# Step 7: Save the combined dataset to a CSV\n",
    "combined_csv_path = os.path.join(folder_name, \"combined_news_dataset.csv\")\n",
    "df.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "print(f\"✅ Combined dataset saved to: {combined_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2434240f-5fbd-4e19-bb98-5ad4fe19187f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the extracted folder:\n",
      "BuzzFeed_fake_news_content.csv\n",
      "BuzzFeed_real_news_content.csv\n",
      "combined_news_dataset.csv\n",
      "PolitiFact_fake_news_content.csv\n",
      "PolitiFact_real_news_content.csv\n",
      "Loading dataset: BuzzFeed_fake_news_content.csv\n",
      "Columns in BuzzFeed_fake_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Loading dataset: PolitiFact_fake_news_content.csv\n",
      "Columns in PolitiFact_fake_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Loading dataset: BuzzFeed_real_news_content.csv\n",
      "Columns in BuzzFeed_real_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Loading dataset: PolitiFact_real_news_content.csv\n",
      "Columns in PolitiFact_real_news_content.csv: ['id', 'title', 'text', 'url', 'top_img', 'authors', 'source', 'publish_date', 'movies', 'images', 'canonical_link', 'meta_data']\n",
      "Sample rows from the combined dataset:\n",
      "                                                text  label\n",
      "0  I woke up this morning to find a variation of ...      1\n",
      "1  Former President Bill Clinton and his Clinton ...      1\n",
      "2  After collapsing just before trying to step in...      1\n",
      "3  Donald Trump is, well, deplorable. He’s sugges...      1\n",
      "4                    Website is Down For Maintenance      1\n",
      "\n",
      "Accuracy: 0.31496062992125984\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.40      0.35        58\n",
      "           1       0.33      0.25      0.28        69\n",
      "\n",
      "    accuracy                           0.31       127\n",
      "   macro avg       0.32      0.32      0.31       127\n",
      "weighted avg       0.32      0.31      0.31       127\n",
      "\n",
      "✅ Decision Tree classifier model and vectorizer saved successfully!\n",
      "✅ Combined dataset saved to: Combined_News_Datasets\\combined_news_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pickle\n",
    "\n",
    "# Step 1: Define the folder path\n",
    "folder_name = \"Combined_News_Datasets\"  # Folder containing your datasets\n",
    "\n",
    "# Step 2: Verify the files in the folder\n",
    "extracted_files = os.listdir(folder_name)\n",
    "print(\"Files in the extracted folder:\")\n",
    "for file in extracted_files:\n",
    "    print(file)\n",
    "\n",
    "# Step 3: Load and label the datasets\n",
    "dataset_files = {\n",
    "    \"BuzzFeed_fake_news_content.csv\": 1,\n",
    "    \"PolitiFact_fake_news_content.csv\": 1,\n",
    "    \"BuzzFeed_real_news_content.csv\": 0,\n",
    "    \"PolitiFact_real_news_content.csv\": 0,\n",
    "}\n",
    "\n",
    "datasets = []\n",
    "for file, label in dataset_files.items():\n",
    "    file_path = os.path.join(folder_name, file)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading dataset: {file}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Columns in {file}: {df.columns.tolist()}\")\n",
    "        \n",
    "        if 'text' in df.columns:\n",
    "            df = df[['text']]\n",
    "            df['label'] = label\n",
    "            datasets.append(df)\n",
    "        else:\n",
    "            print(f\"⚠️ 'text' column not found in {file}. Skipping this file.\")\n",
    "    else:\n",
    "        print(f\"⚠️ Dataset not found: {file}\")\n",
    "\n",
    "# Step 4: Combine all datasets into one DataFrame\n",
    "df = pd.concat(datasets, ignore_index=True)\n",
    "\n",
    "# Step 5: Handle missing values\n",
    "df = df.dropna(subset=['text', 'label'])\n",
    "\n",
    "print(\"Sample rows from the combined dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 6: Prepare data for model training\n",
    "X = df['text'].astype(str)\n",
    "y = df['label']\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the Decision Tree classifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the model and vectorizer\n",
    "with open('fake_news_model_decision_tree.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "    pickle.dump(vectorizer, model_file)\n",
    "\n",
    "print(\"✅ Decision Tree classifier model and vectorizer saved successfully!\")\n",
    "\n",
    "# Step 7: Save the combined dataset to a CSV\n",
    "combined_csv_path = os.path.join(folder_name, \"combined_news_dataset.csv\")\n",
    "df.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "print(f\"✅ Combined dataset saved to: {combined_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029c49bb-f360-4540-9286-a07d79d3cd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
